import torch
import numpy as np
import cv2
import decord
from decord import VideoReader, cpu
import json
import os
from tqdm import tqdm
import tempfile
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import threading

# ê¸°ì¡´ VideoAnomalyDetector í´ë˜ìŠ¤ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
from models.stead_model import Model
import torch.nn.functional as F

class VideoAnomalyDetector:
    def __init__(self, model_path, model_arch='tiny', device='cuda'):
        """
        ë¹„ë””ì˜¤ ì´ìƒ ìƒí™© íƒì§€ ëª¨ë¸ ë˜í¼
        
        Args:
            model_path (str): ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ (.pkl)
            model_arch (str): ëª¨ë¸ ì•„í‚¤í…ì²˜ ('base', 'fast', 'tiny')
            device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda', 'cpu')
        """
        self.device = torch.device(device)
        self.model = self._load_model(model_path, model_arch)
        
    def _load_model(self, model_path, model_arch):
        """ëª¨ë¸ ë¡œë“œ"""
        if model_arch == 'base':
            model = Model()
        elif model_arch in ['fast', 'tiny']:
            model = Model(ff_mult=1, dims=(32,32), depths=(1,1))
        else:
            raise ValueError(f"Unknown model architecture: {model_arch}")
            
        model.load_state_dict(torch.load(model_path, map_location=self.device))
        model.to(self.device)
        model.eval()
        return model
    
    def preprocess_video_tensor(self, video_frames, n_frames=192, target_size=(16, 10, 10)):
        """
        ë¹„ë””ì˜¤ í”„ë ˆì„ í…ì„œ ì „ì²˜ë¦¬
        
        Args:
            video_frames (np.ndarray): ë¹„ë””ì˜¤ í”„ë ˆì„ ë°°ì—´ [T, H, W, C]
            n_frames (int): ìƒ˜í”Œë§í•  í”„ë ˆì„ ìˆ˜
            target_size (tuple): ëª©í‘œ í¬ê¸° (depth, height, width)
            
        Returns:
            torch.Tensor: ì „ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ í…ì„œ [1, n_frames, depth, height, width]
        """
        total_frames = len(video_frames)
        
        # í”„ë ˆì„ ì¸ë±ìŠ¤ ê³„ì‚° (ê· ë“± ìƒ˜í”Œë§)
        if total_frames > n_frames:
            frame_indices = np.linspace(0, total_frames-1, n_frames, dtype=int)
        else:
            # í”„ë ˆì„ì´ ë¶€ì¡±í•˜ë©´ ë°˜ë³µí•´ì„œ ì±„ì›€
            frame_indices = np.tile(np.arange(total_frames), 
                                  (n_frames // total_frames) + 1)[:n_frames]
        
        frames = []
        for frame_idx in frame_indices:
            frame = video_frames[frame_idx]
            
            # í”„ë ˆì„ ì „ì²˜ë¦¬
            if frame.shape[:2] != (target_size[1], target_size[2]):
                frame = cv2.resize(frame, (target_size[2], target_size[1]))  # (W, H)
            
            # BGR to RGB ë³€í™˜ (OpenCVëŠ” BGR, decordëŠ” RGB)
            if frame.shape[-1] == 3:  # RGB ì±„ë„ì´ ìˆëŠ” ê²½ìš°
                frame = frame.astype(np.float32) / 255.0  # ì •ê·œí™”
            
            frames.append(frame)
        
        if len(frames) == 0:
            raise ValueError("ìœ íš¨í•œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë¶€ì¡±í•œ í”„ë ˆì„ì„ ë§ˆì§€ë§‰ í”„ë ˆì„ìœ¼ë¡œ ì±„ì›€
        while len(frames) < n_frames:
            frames.append(frames[-1])
        
        # numpy arrayë¡œ ë³€í™˜: [n_frames, H, W, C]
        video_array = np.stack(frames[:n_frames])
        
        # ì°¨ì› ì¡°ì •: [n_frames, H, W, C] -> [n_frames, C, H, W]
        video_array = video_array.transpose(0, 3, 1, 2)
        
        # depth ì°¨ì› ì¶”ê°€ ë° ì¡°ì •
        if target_size[0] != video_array.shape[1]:
            video_array = np.repeat(video_array, 
                                  target_size[0] // video_array.shape[1] + 1, 
                                  axis=1)[:, :target_size[0], :, :]
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€: [1, n_frames, depth, H, W]
        video_tensor = torch.from_numpy(video_array).unsqueeze(0).float()
        
        return video_tensor
    
    def predict_tensor(self, video_tensor):
        """
        ì´ìƒ ìƒí™© ì˜ˆì¸¡ (í…ì„œ ì…ë ¥)
        
        Args:
            video_tensor (torch.Tensor): ì „ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ í…ì„œ
            
        Returns:
            tuple: (anomaly_score, features)
        """
        with torch.no_grad():
            video_tensor = video_tensor.to(self.device)
            scores, features = self.model(video_tensor)
            
            # ì‹œê·¸ëª¨ì´ë“œ ì ìš©í•˜ì—¬ í™•ë¥ ë¡œ ë³€í™˜
            anomaly_score = torch.sigmoid(scores).squeeze().cpu().item()
            features = features.squeeze().cpu().numpy()
        
        return anomaly_score, features
    
    def predict_batch_tensors(self, video_tensors):
        """
        ë°°ì¹˜ë¡œ ì—¬ëŸ¬ í…ì„œ ì˜ˆì¸¡ (GPU ìµœì í™”)
        
        Args:
            video_tensors (list): ì „ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ í…ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            list: ì´ìƒìƒí™© ìŠ¤ì½”ì–´ ë¦¬ìŠ¤íŠ¸
        """
        if not video_tensors:
            return []
        
        # í…ì„œë“¤ì„ ë°°ì¹˜ë¡œ ê²°í•©
        batch_tensor = torch.cat(video_tensors, dim=0).to(self.device)
        
        with torch.no_grad():
            scores, _ = self.model(batch_tensor)
            # ì‹œê·¸ëª¨ì´ë“œ ì ìš©í•˜ì—¬ í™•ë¥ ë¡œ ë³€í™˜
            anomaly_scores = torch.sigmoid(scores).squeeze().cpu().tolist()
            
            # ë‹¨ì¼ ìŠ¤ì½”ì–´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            if not isinstance(anomaly_scores, list):
                anomaly_scores = [anomaly_scores]
        
        return anomaly_scores


class SegmentAnomalyProcessor:
    def __init__(self, model_path, model_arch='tiny', device='cuda', video_base_path='', batch_size=8):
        """
        ì„¸ê·¸ë¨¼íŠ¸ë³„ ì´ìƒìƒí™© ìŠ¤ì½”ì–´ ê³„ì‚°ê¸°
        
        Args:
            model_path (str): ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ
            model_arch (str): ëª¨ë¸ ì•„í‚¤í…ì²˜
            device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
            video_base_path (str): ë¹„ë””ì˜¤ íŒŒì¼ë“¤ì˜ ê¸°ë³¸ ê²½ë¡œ
            batch_size (int): GPU ë°°ì¹˜ í¬ê¸°
        """
        self.detector = VideoAnomalyDetector(model_path, model_arch, device)
        self.video_base_path = video_base_path
        self.batch_size = batch_size
        
    def calculate_not_segments(self, total_frames, segments):
        """
        ì „ì²´ í”„ë ˆì„ì—ì„œ segmentsë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ êµ¬ê°„ë“¤ ê³„ì‚°
        
        Args:
            total_frames (int): ì „ì²´ í”„ë ˆì„ ìˆ˜
            segments (list): ê¸°ì¡´ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ [[start, end], ...]
            
        Returns:
            list: not_segments ë¦¬ìŠ¤íŠ¸ [[start, end], ...]
        """
        if not segments:
            return [[0, total_frames - 1]]
        
        # ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ì‹œì‘ì  ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        segments = sorted(segments, key=lambda x: x[0])
        
        not_segments = []
        current_pos = 0
        
        for start, end in segments:
            # í˜„ì¬ ìœ„ì¹˜ì™€ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ì  ì‚¬ì´ì— gapì´ ìˆìœ¼ë©´ not_segment ì¶”ê°€
            if current_pos < start:
                not_segments.append([current_pos, start - 1])
            
            # ë‹¤ìŒ ìœ„ì¹˜ ì—…ë°ì´íŠ¸
            current_pos = max(current_pos, end + 1)
        
        # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ì´í›„ì— í”„ë ˆì„ì´ ë‚¨ì•„ìˆìœ¼ë©´ not_segment ì¶”ê°€
        if current_pos < total_frames:
            not_segments.append([current_pos, total_frames - 1])
        
        return not_segments
    
    def extract_segment_frames_decord(self, video_path, start_frame, end_frame):
        """
        Decordë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì„¸ê·¸ë¨¼íŠ¸ì˜ í”„ë ˆì„ ì¶”ì¶œ
        
        Args:
            video_path (str): ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            start_frame (int): ì‹œì‘ í”„ë ˆì„ ì¸ë±ìŠ¤
            end_frame (int): ì¢…ë£Œ í”„ë ˆì„ ì¸ë±ìŠ¤
            
        Returns:
            np.ndarray: í”„ë ˆì„ ë°°ì—´ [T, H, W, C]
        """
        try:
            vr = VideoReader(video_path, ctx=cpu(0))
            total_frames = len(vr)
            
            # í”„ë ˆì„ ì¸ë±ìŠ¤ ì¡°ì •
            start_frame = max(0, start_frame)
            end_frame = min(total_frames - 1, end_frame)
            
            if start_frame >= end_frame:
                raise ValueError(f"Invalid frame range: {start_frame}-{end_frame}")
            
            # í”„ë ˆì„ ì¶”ì¶œ
            frame_indices = list(range(start_frame, end_frame + 1))
            frames = vr.get_batch(frame_indices).asnumpy()  # [T, H, W, C]
            
            return frames
            
        except Exception as e:
            print(f"Decord ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return self.extract_segment_frames_opencv(video_path, start_frame, end_frame)
    
    def extract_segment_frames_opencv(self, video_path, start_frame, end_frame):
        """
        OpenCVë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì„¸ê·¸ë¨¼íŠ¸ì˜ í”„ë ˆì„ ì¶”ì¶œ (fallback)
        
        Args:
            video_path (str): ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            start_frame (int): ì‹œì‘ í”„ë ˆì„ ì¸ë±ìŠ¤
            end_frame (int): ì¢…ë£Œ í”„ë ˆì„ ì¸ë±ìŠ¤
            
        Returns:
            np.ndarray: í”„ë ˆì„ ë°°ì—´ [T, H, W, C]
        """
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            raise ValueError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        start_frame = max(0, start_frame)
        end_frame = min(total_frames - 1, end_frame)
        
        frames = []
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        
        for frame_idx in range(start_frame, end_frame + 1):
            ret, frame = cap.read()
            if not ret:
                break
            
            # BGR to RGB ë³€í™˜
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame)
        
        cap.release()
        
        if len(frames) == 0:
            raise ValueError("ìœ íš¨í•œ í”„ë ˆì„ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return np.array(frames)  # [T, H, W, C]
    
    def extract_all_segments_parallel(self, video_path, all_segments):
        """
        ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ì˜ í”„ë ˆì„ì„ ë³‘ë ¬ë¡œ ì¶”ì¶œ
        
        Args:
            video_path (str): ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            all_segments (list): ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            list: ì¶”ì¶œëœ í”„ë ˆì„ ë°°ì—´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        def extract_single_segment(segment):
            start_frame, end_frame = segment
            return self.extract_segment_frames_decord(video_path, start_frame, end_frame)
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
        with ThreadPoolExecutor(max_workers=4) as executor:
            frames_list = list(executor.map(extract_single_segment, all_segments))
        
        return frames_list
    
    def calculate_all_scores(self, video_data):
        """
        íŠ¹ì • ë¹„ë””ì˜¤ì˜ ëª¨ë“  êµ¬ê°„(segments + not_segments)ì— ëŒ€í•œ ì´ìƒìƒí™© ìŠ¤ì½”ì–´ ê³„ì‚°
        
        Args:
            video_data (dict): ë¹„ë””ì˜¤ ì •ë³´ (video_id, n_frames, segments ë“±)
            
        Returns:
            dict: ê³„ì‚°ëœ ìŠ¤ì½”ì–´ë“¤ê³¼ êµ¬ê°„ ì •ë³´
        """
        video_id = video_data['video_id']
        n_frames = video_data['n_frames']
        segments = video_data['segments']
        
        # ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ êµ¬ì„±
        video_path = os.path.join(self.video_base_path, f"{video_id}.mp4")
        if not os.path.exists(video_path):
            print(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            return {
                'anomaly_segment_score': [0.0] * len(segments),
                'not_segments': [],
                'not_segment_score': [],
                'all_scores': [0.0] * len(segments)
            }
        
        # not_segments ê³„ì‚°
        not_segments = self.calculate_not_segments(n_frames, segments)
        
        # ëª¨ë“  êµ¬ê°„ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ê²°í•© (ìˆœì„œì™€ íƒ€ì… ì •ë³´ ìœ ì§€)
        all_segments_with_type = []
        for i, segment in enumerate(segments):
            all_segments_with_type.append((segment, 'segment', i))
        for i, segment in enumerate(not_segments):
            all_segments_with_type.append((segment, 'not_segment', i))
        
        # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        all_segments_with_type.sort(key=lambda x: x[0][0])
        
        try:
            # ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ì˜ í”„ë ˆì„ ì¶”ì¶œ
            all_segments = [item[0] for item in all_segments_with_type]
            frames_list = self.extract_all_segments_parallel(video_path, all_segments)
            
            # í…ì„œë¡œ ë³€í™˜ (ë°°ì¹˜ ì²˜ë¦¬ìš©)
            video_tensors = []
            valid_indices = []
            
            for i, frames in enumerate(frames_list):
                if frames is not None and len(frames) > 0:
                    try:
                        tensor = self.detector.preprocess_video_tensor(frames)
                        video_tensors.append(tensor)
                        valid_indices.append(i)
                    except Exception as e:
                        print(f"í…ì„œ ë³€í™˜ ì˜¤ë¥˜: {e}")
            
            # ë°°ì¹˜ë¡œ ì˜ˆì¸¡ (GPU ìµœì í™”)
            if video_tensors:
                # ë°°ì¹˜ í¬ê¸°ì— ë”°ë¼ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
                all_scores_raw = []
                for i in range(0, len(video_tensors), self.batch_size):
                    batch = video_tensors[i:i+self.batch_size]
                    batch_scores = self.detector.predict_batch_tensors(batch)
                    all_scores_raw.extend(batch_scores)
            else:
                all_scores_raw = []
            
            # ê²°ê³¼ ì •ë¦¬
            segment_scores = [0.0] * len(segments)
            not_segment_scores = [0.0] * len(not_segments)
            all_scores = [0.0] * len(all_segments_with_type)
            
            # ìœ íš¨í•œ ì¸ë±ìŠ¤ì— ëŒ€í•´ì„œë§Œ ìŠ¤ì½”ì–´ í• ë‹¹
            score_idx = 0
            for i, (segment, seg_type, original_idx) in enumerate(all_segments_with_type):
                if i in valid_indices and score_idx < len(all_scores_raw):
                    score = all_scores_raw[score_idx]
                    score_idx += 1
                    
                    all_scores[i] = score
                    
                    if seg_type == 'segment':
                        segment_scores[original_idx] = score
                    else:  # not_segment
                        not_segment_scores[original_idx] = score
                    
                    print(f"{video_id} - {seg_type.upper()} [{segment[0]}, {segment[1]}]: Score = {score:.4f}")
            
            # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
            print(f"\n=== {video_id} ê²°ê³¼ ìš”ì•½ ===")
            print(f"Segments: {segments}")
            print(f"Segment Scores: {[f'{s:.4f}' for s in segment_scores]}")
            print(f"Not-Segments: {not_segments}")
            print(f"Not-Segment Scores: {[f'{s:.4f}' for s in not_segment_scores]}")
            print(f"All Scores (ì‹œê°„ìˆœ): {[f'{s:.4f}' for s in all_scores]}")
            print("=" * 50)
            
            return {
                'anomaly_segment_score': segment_scores,
                'not_segments': not_segments,
                'not_segment_score': not_segment_scores,
                'all_scores': all_scores
            }
            
        except Exception as e:
            print(f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜ ({video_id}): {e}")
            return {
                'anomaly_segment_score': [0.0] * len(segments),
                'not_segments': not_segments,
                'not_segment_score': [0.0] * len(not_segments),
                'all_scores': [0.0] * (len(segments) + len(not_segments))
            }
    
    def process_jsonl(self, input_jsonl_path, output_jsonl_path):
        """
        JSONL íŒŒì¼ì„ ì½ì–´ì„œ ëª¨ë“  êµ¬ê°„ì˜ ìŠ¤ì½”ì–´ë¥¼ ê³„ì‚°í•˜ê³  ìƒˆ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            input_jsonl_path (str): ì…ë ¥ JSONL íŒŒì¼ ê²½ë¡œ
            output_jsonl_path (str): ì¶œë ¥ JSONL íŒŒì¼ ê²½ë¡œ
        """
        results = []
        
        # JSONL íŒŒì¼ ì½ê¸°
        with open(input_jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line.strip())
                results.append(data)
        
        print(f"ì´ {len(results)}ê°œì˜ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘...")
        print(f"GPU ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        
        # ê° ë¹„ë””ì˜¤ì— ëŒ€í•´ ëª¨ë“  êµ¬ê°„ì˜ ìŠ¤ì½”ì–´ ê³„ì‚°
        for i, video_data in enumerate(tqdm(results, desc="Processing videos")):
            try:
                print(f"\n[{i+1}/{len(results)}] ì²˜ë¦¬ ì¤‘: {video_data['video_id']}")
                scores_data = self.calculate_all_scores(video_data)
                
                # ê²°ê³¼ë¥¼ video_dataì— ì¶”ê°€
                video_data.update(scores_data)
                
                # JSONLì— ì €ì¥ë  ìµœì¢… ë°ì´í„° ì¶œë ¥
                print(f"\nğŸ“ JSONL ì €ì¥ ë°ì´í„°:")
                print(f"  - anomaly_segment_score: {scores_data['anomaly_segment_score']}")
                print(f"  - not_segments: {scores_data['not_segments']}")
                print(f"  - not_segment_score: {scores_data['not_segment_score']}")
                print(f"  - all_scores: {scores_data['all_scores']}")
                print("-" * 70)
                
            except Exception as e:
                print(f"ë¹„ë””ì˜¤ {video_data['video_id']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ì‹œ ê¸°ë³¸ê°’ ì„¤ì •
                segments = video_data.get('segments', [])
                default_data = {
                    'anomaly_segment_score': [0.0] * len(segments),
                    'not_segments': [],
                    'not_segment_score': [],
                    'all_scores': [0.0] * len(segments)
                }
                video_data.update(default_data)
                
                print(f"ğŸ“ JSONL ì €ì¥ ë°ì´í„° (ì˜¤ë¥˜ì‹œ ê¸°ë³¸ê°’):")
                print(f"  - anomaly_segment_score: {default_data['anomaly_segment_score']}")
                print(f"  - not_segments: {default_data['not_segments']}")
                print(f"  - not_segment_score: {default_data['not_segment_score']}")
                print(f"  - all_scores: {default_data['all_scores']}")
                print("-" * 70)
        
        # ê²°ê³¼ë¥¼ ìƒˆ JSONL íŒŒì¼ë¡œ ì €ì¥
        with open(output_jsonl_path, 'w', encoding='utf-8') as f:
            for data in results:
                f.write(json.dumps(data, ensure_ascii=False) + '\n')
        
        print(f"ê²°ê³¼ê°€ {output_jsonl_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def main():
    # ì„¤ì •
    model_path = 'assets/888tiny.pkl'
    model_arch = 'tiny'
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    video_base_path = 'ucf-crime/videos/train'  # ë¹„ë””ì˜¤ íŒŒì¼ë“¤ì´ ìˆëŠ” ê¸°ë³¸ ê²½ë¡œ
    batch_size = 16  # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
    
    input_jsonl = 'result.jsonl'
    output_jsonl = 'result_score.jsonl'
    
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    if device == 'cuda':
        print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    # ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    processor = SegmentAnomalyProcessor(
        model_path=model_path,
        model_arch=model_arch,
        device=device,
        video_base_path=video_base_path,
        batch_size=batch_size
    )
    
    # JSONL ì²˜ë¦¬
    processor.process_jsonl(input_jsonl, output_jsonl)
    
    print("ëª¨ë“  êµ¬ê°„ì˜ ì´ìƒìƒí™© ìŠ¤ì½”ì–´ ê³„ì‚° ì™„ë£Œ!")


if __name__ == "__main__":
    main()